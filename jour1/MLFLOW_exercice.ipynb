{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOCBxgPa0deB",
        "outputId": "bc67bdd2-dceb-490f-faee-6dd3c8e9ecf1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2025.11.12)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjXOVAx40w-Z",
        "outputId": "9c69228b-9346-4c97-b9a9-00bced88e4c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-skinny==3.6.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.6.0 (from mlflow)\n",
            "  Downloading mlflow_tracing-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting Flask-CORS<7 (from mlflow)\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huey<3,>=2.5.0 (from mlflow)\n",
            "  Downloading huey-2.5.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.44)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.6.0->mlflow)\n",
            "  Downloading databricks_sdk-0.73.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.121.3)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (2.11.10)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.38.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.6.0->mlflow) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.6.0->mlflow) (0.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.6.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.6.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (2025.11.12)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.6.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow) (4.11.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.6.0-py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.6.0-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.6.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huey-2.5.4-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.8/76.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.73.0-py3-none-any.whl (753 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m753.9/753.9 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: huey, gunicorn, graphql-core, graphql-relay, docker, graphene, Flask-CORS, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
            "Successfully installed Flask-CORS-6.0.1 databricks-sdk-0.73.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 huey-2.5.4 mlflow-3.6.0 mlflow-skinny-3.6.0 mlflow-tracing-3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n-mzkh_0Ay9",
        "outputId": "6d731b54-743b-4bea-99fe-6b62d2cc7ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All imports successful!\n",
            "ğŸ“¦ Packages loaded:\n",
            "   - pandas: 2.2.2\n",
            "   - numpy: 2.0.2\n",
            "   - mlflow: 3.6.0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "MLflow Lab Exercise: Steel Plates Faults Classification\n",
        "================================================================================\n",
        "\n",
        "OBJECTIVE:\n",
        "Learn how to use MLflow to track machine learning experiments for classifying\n",
        "faults in steel plates. You will train multiple classification models and\n",
        "compare their performance using MLflow's tracking capabilities.\n",
        "\n",
        "DATASET:\n",
        "Steel Plates Faults Dataset\n",
        "- Source: UCI Machine Learning Repository (ID: 198)\n",
        "- Task: Classification (predict fault types in steel plates)\n",
        "- Features: 27 numerical features describing steel plate characteristics\n",
        "- Target: 7 different fault types\n",
        "\n",
        "YOUR TASKS:\n",
        "1. Complete the model hyperparameters (FIX ME sections)\n",
        "2. Implement MLflow tracking for each model\n",
        "3. Log parameters, metrics, and models\n",
        "4. Compare model performance using MLflow\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Replace all \"FIX ME\" placeholders with appropriate values\n",
        "- Run each cell in order\n",
        "- Check MLflow UI at http://localhost:5000 after training\n",
        "\n",
        "Good luck!\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: IMPORTS AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from datetime import datetime\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.tracking import MlflowClient\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n",
        "print(\"ğŸ“¦ Packages loaded:\")\n",
        "print(f\"   - pandas: {pd.__version__}\")\n",
        "print(f\"   - numpy: {np.__version__}\")\n",
        "print(f\"   - mlflow: {mlflow.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: LOAD AND PREPARE DATA USING UCIMLREPO\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ“Š LOADING STEEL PLATES FAULTS DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Install required package first: pip install ucimlrepo\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Fetch dataset\n",
        "steel_plates_faults = fetch_ucirepo(id=198)\n",
        "\n",
        "# Data (as pandas dataframes)\n",
        "X = steel_plates_faults.data.features\n",
        "y = steel_plates_faults.data.targets\n",
        "\n",
        "# Metadata\n",
        "print(\"ğŸ“‹ Dataset Metadata:\")\n",
        "print(f\"   - Name: {steel_plates_faults.metadata['name']}\")\n",
        "print(f\"   - Number of Instances: {steel_plates_faults.metadata['num_instances']}\")\n",
        "print(f\"   - Number of Features: {steel_plates_faults.metadata['num_features']}\")\n",
        "\n",
        "# Variable information\n",
        "print(f\"\\nğŸ“Š Dataset Shape: {X.shape[0]} rows Ã— {X.shape[1]} columns\")\n",
        "\n",
        "# Display basic info\n",
        "print(f\"\\nğŸ“‹ Features Info:\")\n",
        "print(f\"   Feature names: {list(X.columns)}\")\n",
        "\n",
        "# Check target variable\n",
        "print(f\"\\nğŸ¯ Target Variable Information:\")\n",
        "print(f\"   Target columns: {list(y.columns)}\")\n",
        "print(f\"   Target shape: {y.shape}\")\n",
        "\n",
        "# Since the target has multiple columns (one for each fault type), we need to convert to single column\n",
        "# The dataset has 7 binary columns for each fault type\n",
        "print(f\"\\nğŸ” Target value counts for each fault type:\")\n",
        "for col in y.columns:\n",
        "    print(f\"   {col}: {y[col].sum()} samples\")\n",
        "\n",
        "# Convert multi-column target to single column\n",
        "y_single = y.idxmax(axis=1)  # Get the column name with the highest value (1)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_single)\n",
        "\n",
        "print(f\"\\nğŸ” Encoded Target Classes:\")\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    class_count = (y_encoded == i).sum()\n",
        "    percentage = (class_count / len(y_encoded)) * 100\n",
        "    print(f\"   {i}: {class_name} ({class_count} samples, {percentage:.2f}%)\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nğŸ” Missing values in features: {X.isnull().sum().sum()}\")\n",
        "print(f\"ğŸ” Missing values in target: {y.isnull().sum().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws8iri2T0itN",
        "outputId": "8cd94498-f4cf-4b4e-aca3-4656dc2df2eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ğŸ“Š LOADING STEEL PLATES FAULTS DATASET\n",
            "================================================================================\n",
            "ğŸ“‹ Dataset Metadata:\n",
            "   - Name: Steel Plates Faults\n",
            "   - Number of Instances: 1941\n",
            "   - Number of Features: 27\n",
            "\n",
            "ğŸ“Š Dataset Shape: 1941 rows Ã— 27 columns\n",
            "\n",
            "ğŸ“‹ Features Info:\n",
            "   Feature names: ['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas', 'Minimum_of_Luminosity']\n",
            "\n",
            "ğŸ¯ Target Variable Information:\n",
            "   Target columns: ['Pastry', 'Z_Scratch', 'K_Scratch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
            "   Target shape: (1941, 7)\n",
            "\n",
            "ğŸ” Target value counts for each fault type:\n",
            "   Pastry: 158 samples\n",
            "   Z_Scratch: 190 samples\n",
            "   K_Scratch: 391 samples\n",
            "   Stains: 72 samples\n",
            "   Dirtiness: 55 samples\n",
            "   Bumps: 402 samples\n",
            "   Other_Faults: 673 samples\n",
            "\n",
            "ğŸ” Encoded Target Classes:\n",
            "   0: Bumps (402 samples, 20.71%)\n",
            "   1: Dirtiness (55 samples, 2.83%)\n",
            "   2: K_Scratch (391 samples, 20.14%)\n",
            "   3: Other_Faults (673 samples, 34.67%)\n",
            "   4: Pastry (158 samples, 8.14%)\n",
            "   5: Stains (72 samples, 3.71%)\n",
            "   6: Z_Scratch (190 samples, 9.79%)\n",
            "\n",
            "ğŸ” Missing values in features: 0\n",
            "ğŸ” Missing values in target: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: DATA EXPLORATION AND PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ”§ DATA EXPLORATION AND PREPARATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Display basic statistics\n",
        "print(f\"\\nğŸ“Š Features Statistics:\")\n",
        "print(X.describe())\n",
        "\n",
        "# Check class distribution\n",
        "print(f\"\\nğŸ“ˆ Class Distribution:\")\n",
        "class_counts = pd.Series(y_encoded).value_counts().sort_index()\n",
        "for class_idx, count in class_counts.items():\n",
        "    class_name = label_encoder.classes_[class_idx]\n",
        "    percentage = (count / len(y_encoded)) * 100\n",
        "    print(f\"   {class_name}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "# Prepare final dataset\n",
        "print(f\"\\nâœ… Final Dataset Shape:\")\n",
        "print(f\"   Features (X): {X.shape}\")\n",
        "print(f\"   Target (y): {y_encoded.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJsnbsFu16jk",
        "outputId": "9303ae3e-59d4-4d15-c8f0-835b0c2c7830"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ”§ DATA EXPLORATION AND PREPARATION\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Features Statistics:\n",
            "         X_Minimum    X_Maximum     Y_Minimum     Y_Maximum   Pixels_Areas  \\\n",
            "count  1941.000000  1941.000000  1.941000e+03  1.941000e+03    1941.000000   \n",
            "mean    571.136012   617.964451  1.650685e+06  1.650739e+06    1893.878413   \n",
            "std     520.690671   497.627410  1.774578e+06  1.774590e+06    5168.459560   \n",
            "min       0.000000     4.000000  6.712000e+03  6.724000e+03       2.000000   \n",
            "25%      51.000000   192.000000  4.712530e+05  4.712810e+05      84.000000   \n",
            "50%     435.000000   467.000000  1.204128e+06  1.204136e+06     174.000000   \n",
            "75%    1053.000000  1072.000000  2.183073e+06  2.183084e+06     822.000000   \n",
            "max    1705.000000  1713.000000  1.298766e+07  1.298769e+07  152655.000000   \n",
            "\n",
            "        X_Perimeter   Y_Perimeter  Sum_of_Luminosity  Maximum_of_Luminosity  \\\n",
            "count   1941.000000   1941.000000       1.941000e+03            1941.000000   \n",
            "mean     111.855229     82.965997       2.063121e+05             130.193715   \n",
            "std      301.209187    426.482879       5.122936e+05              18.690992   \n",
            "min        2.000000      1.000000       2.500000e+02              37.000000   \n",
            "25%       15.000000     13.000000       9.522000e+03             124.000000   \n",
            "50%       26.000000     25.000000       1.920200e+04             127.000000   \n",
            "75%       84.000000     83.000000       8.301100e+04             140.000000   \n",
            "max    10449.000000  18152.000000       1.159141e+07             253.000000   \n",
            "\n",
            "       Length_of_Conveyer  ...  Edges_X_Index  Edges_Y_Index  \\\n",
            "count         1941.000000  ...    1941.000000    1941.000000   \n",
            "mean          1459.160227  ...       0.610529       0.813472   \n",
            "std            144.577823  ...       0.243277       0.234274   \n",
            "min           1227.000000  ...       0.014400       0.048400   \n",
            "25%           1358.000000  ...       0.411800       0.596800   \n",
            "50%           1364.000000  ...       0.636400       0.947400   \n",
            "75%           1650.000000  ...       0.800000       1.000000   \n",
            "max           1794.000000  ...       1.000000       1.000000   \n",
            "\n",
            "       Outside_Global_Index   LogOfAreas  Log_X_Index  Log_Y_Index  \\\n",
            "count           1941.000000  1941.000000  1941.000000  1941.000000   \n",
            "mean               0.575734     2.492388     1.335686     1.403271   \n",
            "std                0.482352     0.788930     0.481612     0.454345   \n",
            "min                0.000000     0.301000     0.301000     0.000000   \n",
            "25%                0.000000     1.924300     1.000000     1.079200   \n",
            "50%                1.000000     2.240600     1.176100     1.322200   \n",
            "75%                1.000000     2.914900     1.518500     1.732400   \n",
            "max                1.000000     5.183700     3.074100     4.258700   \n",
            "\n",
            "       Orientation_Index  Luminosity_Index  SigmoidOfAreas  \\\n",
            "count        1941.000000       1941.000000     1941.000000   \n",
            "mean            0.083288         -0.131305        0.585420   \n",
            "std             0.500868          0.148767        0.339452   \n",
            "min            -0.991000         -0.998900        0.119000   \n",
            "25%            -0.333300         -0.195000        0.248200   \n",
            "50%             0.095200         -0.133000        0.506300   \n",
            "75%             0.511600         -0.066600        0.999800   \n",
            "max             0.991700          0.642100        1.000000   \n",
            "\n",
            "       Minimum_of_Luminosity  \n",
            "count            1941.000000  \n",
            "mean               84.548686  \n",
            "std                32.134276  \n",
            "min                 0.000000  \n",
            "25%                63.000000  \n",
            "50%                90.000000  \n",
            "75%               106.000000  \n",
            "max               203.000000  \n",
            "\n",
            "[8 rows x 27 columns]\n",
            "\n",
            "ğŸ“ˆ Class Distribution:\n",
            "   Bumps: 402 samples (20.71%)\n",
            "   Dirtiness: 55 samples (2.83%)\n",
            "   K_Scratch: 391 samples (20.14%)\n",
            "   Other_Faults: 673 samples (34.67%)\n",
            "   Pastry: 158 samples (8.14%)\n",
            "   Stains: 72 samples (3.71%)\n",
            "   Z_Scratch: 190 samples (9.79%)\n",
            "\n",
            "âœ… Final Dataset Shape:\n",
            "   Features (X): (1941, 27)\n",
            "   Target (y): (1941,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: DATA SPLITTING AND SCALING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ‚ï¸ DATA SPLITTING AND SCALING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"âœ… Data split:\")\n",
        "print(f\"   Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"   Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# Display class distribution for splits\n",
        "print(f\"\\nğŸ“Š Training set class distribution:\")\n",
        "train_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "for class_idx, count in train_counts.items():\n",
        "    class_name = label_encoder.classes_[class_idx]\n",
        "    percentage = (count / len(y_train)) * 100\n",
        "    print(f\"   {class_name}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Test set class distribution:\")\n",
        "test_counts = pd.Series(y_test).value_counts().sort_index()\n",
        "for class_idx, count in test_counts.items():\n",
        "    class_name = label_encoder.classes_[class_idx]\n",
        "    percentage = (count / len(y_test)) * 100\n",
        "    print(f\"   {class_name}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nâœ… Features scaled using StandardScaler\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tizejrl31-II",
        "outputId": "e66ae933-3395-40d6-d177-cce8ebb5bd25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "âœ‚ï¸ DATA SPLITTING AND SCALING\n",
            "================================================================================\n",
            "âœ… Data split:\n",
            "   Training set: 1552 samples\n",
            "   Test set: 389 samples\n",
            "\n",
            "ğŸ“Š Training set class distribution:\n",
            "   Bumps: 321 samples (20.68%)\n",
            "   Dirtiness: 44 samples (2.84%)\n",
            "   K_Scratch: 313 samples (20.17%)\n",
            "   Other_Faults: 538 samples (34.66%)\n",
            "   Pastry: 126 samples (8.12%)\n",
            "   Stains: 58 samples (3.74%)\n",
            "   Z_Scratch: 152 samples (9.79%)\n",
            "\n",
            "ğŸ“Š Test set class distribution:\n",
            "   Bumps: 81 samples (20.82%)\n",
            "   Dirtiness: 11 samples (2.83%)\n",
            "   K_Scratch: 78 samples (20.05%)\n",
            "   Other_Faults: 135 samples (34.70%)\n",
            "   Pastry: 32 samples (8.23%)\n",
            "   Stains: 14 samples (3.60%)\n",
            "   Z_Scratch: 38 samples (9.77%)\n",
            "\n",
            "âœ… Features scaled using StandardScaler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: MODEL HYPERPARAMETERS - LOGISTIC REGRESSION (FIX ME!)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "FIX ME: Define hyperparameters for Logistic Regression\n",
        "\n",
        "HINTS:\n",
        "- C: Inverse of regularization strength (try values between 0.1 and 10)\n",
        "- penalty: Regularization type ('l1', 'l2', 'elasticnet', 'none')\n",
        "- solver: Algorithm to use ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')\n",
        "- max_iter: Maximum number of iterations (try 100, 200, 500)\n",
        "\"\"\"\n",
        "\n",
        "lr_params = {\n",
        "    'C': \"FIX ME\",  # Regularization strength\n",
        "    'penalty': \"FIX ME\",  # Regularization type\n",
        "    'solver': \"FIX ME\",  # Solver algorithm\n",
        "    'max_iter': \"FIX ME\",  # Maximum iterations\n",
        "    'random_state': 42,\n",
        "    'multi_class': 'auto'\n",
        "}\n",
        "\n",
        "print(\"ğŸ“ Logistic Regression Parameters:\")\n",
        "print(lr_params)"
      ],
      "metadata": {
        "id": "3ohI2nRB2BmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: MODEL HYPERPARAMETERS - RANDOM FOREST (FIX ME!)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "FIX ME: Define hyperparameters for Random Forest Classifier\n",
        "\n",
        "HINTS:\n",
        "- n_estimators: Number of trees (try 50, 100, 200)\n",
        "- max_depth: Maximum tree depth (try 10, 20, 30, None)\n",
        "- min_samples_split: Minimum samples to split (try 2, 5, 10)\n",
        "- min_samples_leaf: Minimum samples at leaf node (try 1, 2, 4)\n",
        "\"\"\"\n",
        "\n",
        "rf_params = {\n",
        "    'n_estimators': \"FIX ME\",  # Number of trees\n",
        "    'max_depth': \"FIX ME\",  # Maximum depth\n",
        "    'min_samples_split': \"FIX ME\",  # Minimum samples to split\n",
        "    'min_samples_leaf': \"FIX ME\",  # Minimum samples at leaf\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "print(\"ğŸŒ² Random Forest Classifier Parameters:\")\n",
        "print(rf_params)"
      ],
      "metadata": {
        "id": "0jpNbkFS2EyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: MODEL HYPERPARAMETERS - GRADIENT BOOSTING (FIX ME!)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "FIX ME: Define hyperparameters for Gradient Boosting Classifier\n",
        "\n",
        "HINTS:\n",
        "- n_estimators: Number of boosting stages (try 100, 200, 300)\n",
        "- learning_rate: Shrinks contribution of each tree (try 0.05, 0.1, 0.2)\n",
        "- max_depth: Maximum tree depth (try 3, 5, 7)\n",
        "- subsample: Fraction of samples for training (try 0.8, 0.9, 1.0)\n",
        "\"\"\"\n",
        "\n",
        "gb_params = {\n",
        "    'n_estimators': \"FIX ME\",  # Number of boosting stages\n",
        "    'learning_rate': \"FIX ME\",  # Learning rate\n",
        "    'max_depth': \"FIX ME\",  # Maximum depth\n",
        "    'subsample': \"FIX ME\",  # Subsample ratio\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "print(\"âš¡ Gradient Boosting Classifier Parameters:\")\n",
        "print(gb_params)"
      ],
      "metadata": {
        "id": "rTs92Zhv2G7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: SETUP MLFLOW EXPERIMENT (FIX ME!)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "FIX ME: Set up MLflow experiment and enable autologging\n",
        "\n",
        "TASKS:\n",
        "1. Set the experiment name using mlflow.set_experiment()\n",
        "2. Enable sklearn autologging using mlflow.sklearn.autolog()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸš€ SETTING UP MLFLOW\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "experiment_name = \"FIX ME\"\n",
        "\n",
        "# FIX ME: Set experiment name\n",
        "\"FIX ME\"\n",
        "\n",
        "# FIX ME: Enable autologging\n",
        "\"FIX ME\"\n",
        "\n",
        "print(f\"âœ… MLflow experiment created: '{experiment_name}'\")\n",
        "print(f\"ğŸ“ Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "print(f\"âœ… MLflow autolog enabled for scikit-learn\")"
      ],
      "metadata": {
        "id": "iMAaatp62J9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: TRAIN AND LOG MODEL FUNCTION (FIX ME!)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "FIX ME: Complete the train_and_log_model function\n",
        "\n",
        "TASKS:\n",
        "1. Start an MLflow run with mlflow.start_run()\n",
        "2. Log parameters using mlflow.log_params()\n",
        "3. Log metrics using mlflow.log_metrics()\n",
        "4. Log the trained model using mlflow.sklearn.log_model()\n",
        "5. Set tags using mlflow.set_tags()\n",
        "\n",
        "HINTS:\n",
        "- Use 'with mlflow.start_run(run_name=model_name) as run:' context\n",
        "- mlflow.log_params(params)\n",
        "- mlflow.log_metrics(metrics_dict)\n",
        "- mlflow.sklearn.log_model(model, \"model\")\n",
        "- mlflow.set_tags({\"key\": \"value\"})\n",
        "\"\"\"\n",
        "\n",
        "def train_and_log_model(model, model_name, params, X_train, X_test, y_train, y_test):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ¯ Training Model: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # FIX ME: Start MLflow run\n",
        "    \"FIX ME\":\n",
        "\n",
        "        print(f\"ğŸ“Š Run ID: {run.info.run_id}\")\n",
        "\n",
        "        # FIX ME: Log parameters\n",
        "        \"FIX ME\"\n",
        "        print(f\"âœ… Parameters logged: {params}\")\n",
        "\n",
        "        # Train model\n",
        "        start_time = datetime.now()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        y_pred_proba_test = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "        # Calculate classification metrics\n",
        "        metrics = {\n",
        "            # Training metrics\n",
        "            'train_accuracy': accuracy_score(y_train, y_pred_train),\n",
        "            'train_precision_macro': precision_score(y_train, y_pred_train, average='macro', zero_division=0),\n",
        "            'train_recall_macro': recall_score(y_train, y_pred_train, average='macro', zero_division=0),\n",
        "            'train_f1_macro': f1_score(y_train, y_pred_train, average='macro', zero_division=0),\n",
        "\n",
        "            # Test metrics\n",
        "            'test_accuracy': accuracy_score(y_test, y_pred_test),\n",
        "            'test_precision_macro': precision_score(y_test, y_pred_test, average='macro', zero_division=0),\n",
        "            'test_recall_macro': recall_score(y_test, y_pred_test, average='macro', zero_division=0),\n",
        "            'test_f1_macro': f1_score(y_test, y_pred_test, average='macro', zero_division=0),\n",
        "\n",
        "            # Training time\n",
        "            'training_time_seconds': training_time\n",
        "        }\n",
        "\n",
        "        # FIX ME: Log metrics\n",
        "        \"FIX ME\"\n",
        "\n",
        "        print(f\"âœ… Metrics logged:\")\n",
        "        print(f\"\\n   Training Metrics:\")\n",
        "        print(f\"   - Accuracy: {metrics['train_accuracy']:.4f}\")\n",
        "        print(f\"   - Precision (macro): {metrics['train_precision_macro']:.4f}\")\n",
        "        print(f\"   - Recall (macro): {metrics['train_recall_macro']:.4f}\")\n",
        "        print(f\"   - F1 (macro): {metrics['train_f1_macro']:.4f}\")\n",
        "        print(f\"\\n   Test Metrics:\")\n",
        "        print(f\"   - Accuracy: {metrics['test_accuracy']:.4f}\")\n",
        "        print(f\"   - Precision (macro): {metrics['test_precision_macro']:.4f}\")\n",
        "        print(f\"   - Recall (macro): {metrics['test_recall_macro']:.4f}\")\n",
        "        print(f\"   - F1 (macro): {metrics['test_f1_macro']:.4f}\")\n",
        "        print(f\"\\n   Training Time: {training_time:.2f}s\")\n",
        "\n",
        "        # FIX ME: Log model\n",
        "        \"FIX ME\"\n",
        "        print(f\"âœ… Model artifact saved\")\n",
        "\n",
        "        # FIX ME: Set tags\n",
        "        \"FIX ME\"\n",
        "        print(f\"âœ… Tags added for easy filtering\")\n",
        "\n",
        "        print(f\"\\nğŸ‰ Run completed successfully!\")\n",
        "\n",
        "        return run.info.run_id, metrics\n",
        "\n",
        "print(\"âœ… Training function defined!\")"
      ],
      "metadata": {
        "id": "BG2G1_Z82Njd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10: TRAIN LOGISTIC REGRESSION (FIX ME!)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "FIX ME: Train Logistic Regression with MLflow tracking\n",
        "\n",
        "TASKS:\n",
        "1. Create LogisticRegression instance with lr_params\n",
        "2. Call train_and_log_model() with appropriate arguments\n",
        "\n",
        "HINTS:\n",
        "- lr_model = LogisticRegression(**lr_params)\n",
        "- train_and_log_model(lr_model, \"Logistic_Regression\", lr_params, X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ“ MODEL 1: LOGISTIC REGRESSION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# FIX ME: Create model instance\n",
        "lr_model = \"FIX ME\"\n",
        "\n",
        "# FIX ME: Train and log model\n",
        "lr_run_id, lr_metrics = \"FIX ME\"\n",
        "\n",
        "print(f\"\\nâœ… Logistic Regression training complete!\")\n",
        "print(f\"   Run ID: {lr_run_id}\")\n",
        "print(f\"   Test Accuracy: {lr_metrics['test_accuracy']:.4f}\")\n",
        "print(f\"   Test F1 Score: {lr_metrics['test_f1_macro']:.4f}\")"
      ],
      "metadata": {
        "id": "HvOK0O0J2Vhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: TRAIN RANDOM FOREST (FIX ME!)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "FIX ME: Train Random Forest with MLflow tracking\n",
        "\n",
        "TASKS:\n",
        "1. Create RandomForestClassifier instance with rf_params\n",
        "2. Call train_and_log_model() with appropriate arguments\n",
        "\n",
        "HINTS:\n",
        "- rf_model = RandomForestClassifier(**rf_params)\n",
        "- train_and_log_model(rf_model, \"Random_Forest\", rf_params, X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸŒ² MODEL 2: RANDOM FOREST CLASSIFIER\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# FIX ME: Create model instance\n",
        "rf_model = \"FIX ME\"\n",
        "\n",
        "# FIX ME: Train and log model\n",
        "rf_run_id, rf_metrics = \"FIX ME\"\n",
        "\n",
        "print(f\"\\nâœ… Random Forest training complete!\")\n",
        "print(f\"   Run ID: {rf_run_id}\")\n",
        "print(f\"   Test Accuracy: {rf_metrics['test_accuracy']:.4f}\")\n",
        "print(f\"   Test F1 Score: {rf_metrics['test_f1_macro']:.4f}\")"
      ],
      "metadata": {
        "id": "1ycmqThf2YFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 12: TRAIN GRADIENT BOOSTING (FIX ME!)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "FIX ME: Train Gradient Boosting with MLflow tracking\n",
        "\n",
        "TASKS:\n",
        "1. Create GradientBoostingClassifier instance with gb_params\n",
        "2. Call train_and_log_model() with appropriate arguments\n",
        "\n",
        "HINTS:\n",
        "- gb_model = GradientBoostingClassifier(**gb_params)\n",
        "- train_and_log_model(gb_model, \"Gradient_Boosting\", gb_params, X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âš¡ MODEL 3: GRADIENT BOOSTING CLASSIFIER\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# FIX ME: Create model instance\n",
        "gb_model = \"FIX ME\"\n",
        "\n",
        "# FIX ME: Train and log model\n",
        "gb_run_id, gb_metrics = \"FIX ME\"\n",
        "\n",
        "print(f\"\\nâœ… Gradient Boosting training complete!\")\n",
        "print(f\"   Run ID: {gb_run_id}\")\n",
        "print(f\"   Test Accuracy: {gb_metrics['test_accuracy']:.4f}\")\n",
        "print(f\"   Test F1 Score: {gb_metrics['test_f1_macro']:.4f}\")"
      ],
      "metadata": {
        "id": "ZgVlQu-F2bBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 13: COMPARE RESULTS FROM MLFLOW\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ“Š COMPARING RESULTS FROM MLFLOW\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get experiment and runs\n",
        "client = MlflowClient()\n",
        "experiment = client.get_experiment_by_name(experiment_name)\n",
        "runs = client.search_runs(\n",
        "    experiment_ids=[experiment.experiment_id],\n",
        "    order_by=[\"metrics.test_accuracy DESC\"]\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Found {len(runs)} runs in experiment '{experiment_name}'\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "mlflow_results = []\n",
        "for run in runs:\n",
        "    mlflow_results.append({\n",
        "        'Model': run.data.tags.get('mlflow.runName', 'Unknown'),\n",
        "        'Test_Accuracy': run.data.metrics.get('test_accuracy', 0),\n",
        "        'Test_Precision': run.data.metrics.get('test_precision_macro', 0),\n",
        "        'Test_Recall': run.data.metrics.get('test_recall_macro', 0),\n",
        "        'Test_F1_Score': run.data.metrics.get('test_f1_macro', 0),\n",
        "        'Train_Accuracy': run.data.metrics.get('train_accuracy', 0),\n",
        "        'Training_Time': run.data.metrics.get('training_time_seconds', 0),\n",
        "        'Run_ID': run.info.run_id[:8] + '...'\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(mlflow_results)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ† FINAL RESULTS (Sorted by Test Accuracy)\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "if len(results_df) > 0:\n",
        "    best_model = results_df.loc[results_df['Test_Accuracy'].idxmax()]\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ğŸ¥‡ BEST MODEL (Highest Test Accuracy)\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Model: {best_model['Model']}\")\n",
        "    print(f\"Test Accuracy: {best_model['Test_Accuracy']:.4f}\")\n",
        "    print(f\"Test Precision: {best_model['Test_Precision']:.4f}\")\n",
        "    print(f\"Test Recall: {best_model['Test_Recall']:.4f}\")\n",
        "    print(f\"Test F1 Score: {best_model['Test_F1_Score']:.4f}\")\n",
        "    print(f\"Training Time: {best_model['Training_Time']:.2f}s\")\n",
        "    print(f\"Run ID: {best_model['Run_ID']}\")\n"
      ],
      "metadata": {
        "id": "dA1HjzkG2dSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 14: LAUNCH MLFLOW UI (OPTIONAL)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "To view the MLflow UI, run this command in your terminal:\n",
        "\n",
        "    mlflow ui --host 0.0.0.0 --port 5000\n",
        "\n",
        "Then open your browser and navigate to: http://localhost:5000\n",
        "\n",
        "In the UI you can:\n",
        "- Compare model performance visually\n",
        "- View parameter combinations\n",
        "- Download trained models\n",
        "- Track experiment history\n",
        "- Filter and search runs\n",
        "- Analyze classification results\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ‰ LAB COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nğŸ“ˆ Key Takeaways:\")\n",
        "print(\"   - Trained 3 classification models for steel plates faults detection\")\n",
        "print(\"   - Used MLflow to track all experiments automatically\")\n",
        "print(\"   - Logged parameters, metrics, and trained models\")\n",
        "print(\"   - Compared models using multiple classification metrics\")\n",
        "print(\"   - Can now reproduce results and deploy best model\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Run in terminal: mlflow ui --host 0.0.0.0 --port 5000\")\n",
        "print(\"2. Open: http://localhost:5000 in your browser\")\n",
        "print(\"3. Explore your experiments in the MLflow UI\")\n",
        "print(\"4. Try different hyperparameters to improve performance!\")\n",
        "print(\"\\nğŸ’¡ Challenge: Can you get Test Accuracy above 0.85?\")\n",
        "print(\"   Hint: Try adjusting n_estimators, learning_rate, or max_depth\")\n",
        "print(\"   Consider feature engineering or ensemble methods!\")\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "cu5CQ2jv2hpz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}